{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8IA-DeepLearning-Aula3-MLP","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"VOEB6B5SE83i","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Thu May 30 16:05:38 2019\n","\n","@author: felip\n","\"\"\"\n","import pandas as pd\n","from pandas import Series, DataFrame\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics \n","from sklearn.neural_network import MLPClassifier\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","\n","url = 'https://raw.githubusercontent.com/BigDataGal/Python-for-Data-Science/master/titanic-train.csv'\n","titanic = pd.read_csv(url)\n","titanic.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n","titanic_data = titanic.drop(['PassengerId','Name','Ticket','Cabin'], 1)\n","\n","def age_approx(cols):\n","    Age = cols[0]\n","    Pclass = cols[1]\n","    \n","    if pd.isnull(Age):\n","        if Pclass == 1:\n","            return 37\n","        elif Pclass == 2:\n","            return 29\n","        else:\n","            return 24\n","    else:\n","        return Age"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"semrNHkBFH4m","colab_type":"code","colab":{}},"source":["titanic_data['Age'] = titanic_data[['Age', 'Pclass']].apply(age_approx, axis=1)\n","titanic_data.dropna(inplace=True)\n","gender = pd.get_dummies(titanic_data['Sex'],drop_first=True)\n","embark_location = pd.get_dummies(titanic_data['Embarked'],drop_first=True)\n","titanic_data.drop(['Sex', 'Embarked'],axis=1,inplace=True)\n","titanic_dmy = pd.concat([titanic_data,gender,embark_location],axis=1)\n","titanic_dmy.drop(['Pclass'],axis=1,inplace=True)\n","titanic_dmy.drop(['Q'],axis=1,inplace=True)\n","\n","X = titanic_dmy.iloc[:,[1,2,3,4,5,6]].values\n","y = titanic_dmy.iloc[:,0].values\n","\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=25)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Act9KbcUFK_s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"5f5758a6-0640-4582-cf31-8a38c5899ae2","executionInfo":{"status":"ok","timestamp":1573318345372,"user_tz":180,"elapsed":592,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=200,learning_rate_init=0.01, hidden_layer_sizes=(2,2), activation='logistic', solver='sgd' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      1.00      0.76       164\n","           1       0.00      0.00      0.00       103\n","\n","    accuracy                           0.61       267\n","   macro avg       0.31      0.50      0.38       267\n","weighted avg       0.38      0.61      0.47       267\n","\n","0.6142322097378277\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yDzER-q0HtT1","colab_type":"text"},"source":["Teste 1"]},{"cell_type":"code","metadata":{"id":"vgCBY9n0HuE8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"fb235e41-8cce-400a-8698-05f6bd8cf14b","executionInfo":{"status":"ok","timestamp":1573319040537,"user_tz":180,"elapsed":1883,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=200,learning_rate_init=0.01, hidden_layer_sizes=(5,4,3,2), activation='relu', solver='sgd' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.84      0.83       164\n","           1       0.73      0.70      0.72       103\n","\n","    accuracy                           0.79       267\n","   macro avg       0.78      0.77      0.77       267\n","weighted avg       0.78      0.79      0.79       267\n","\n","0.7865168539325843\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"F1khLNL0H2UN","colab_type":"text"},"source":["Teste 2"]},{"cell_type":"code","metadata":{"id":"8gBQ-OuUH1aM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"415fde5a-9fc8-4461-a4a7-65b6bdb4da9f","executionInfo":{"status":"ok","timestamp":1573319127625,"user_tz":180,"elapsed":785,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=200,learning_rate_init=0.01, hidden_layer_sizes=(20,20), activation='logistic', solver='sgd' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.71      0.97      0.82       164\n","           1       0.88      0.36      0.51       103\n","\n","    accuracy                           0.73       267\n","   macro avg       0.79      0.66      0.66       267\n","weighted avg       0.77      0.73      0.70       267\n","\n","0.7340823970037453\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"oxgM4i9bIMK1","colab_type":"text"},"source":["Teste 3"]},{"cell_type":"code","metadata":{"id":"hSQ2pVxBIIcs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"5c6507d8-73f4-4e84-dddd-55a48315f071","executionInfo":{"status":"ok","timestamp":1573319174039,"user_tz":180,"elapsed":544,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=1000,learning_rate_init=0.01, hidden_layer_sizes=(20,20), activation='logistic', solver='sgd' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.61      1.00      0.76       164\n","           1       0.00      0.00      0.00       103\n","\n","    accuracy                           0.61       267\n","   macro avg       0.31      0.50      0.38       267\n","weighted avg       0.38      0.61      0.47       267\n","\n","0.6142322097378277\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9ygaSm8SIbgO","colab_type":"text"},"source":["Teste 4"]},{"cell_type":"code","metadata":{"id":"lJ8ilgtDIWU8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"fd6faf78-f98b-4d0b-8458-43b1348d2033","executionInfo":{"status":"ok","timestamp":1573319231041,"user_tz":180,"elapsed":1421,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=1000,learning_rate_init=0.01, hidden_layer_sizes=(20,20), activation='tanh', solver='adam' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.84      0.83      0.83       164\n","           1       0.73      0.75      0.74       103\n","\n","    accuracy                           0.80       267\n","   macro avg       0.79      0.79      0.79       267\n","weighted avg       0.80      0.80      0.80       267\n","\n","0.797752808988764\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ep-gM_RfIut2","colab_type":"text"},"source":["Teste 5"]},{"cell_type":"code","metadata":{"id":"psvZvlLYIiWs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"e44011d4-c13c-4c78-f8c7-c5e2a235446b","executionInfo":{"status":"ok","timestamp":1573319288411,"user_tz":180,"elapsed":662,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=1000,learning_rate_init=0.01, hidden_layer_sizes=(2,2), activation='tanh', solver='adam' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.85      0.84       164\n","           1       0.76      0.72      0.74       103\n","\n","    accuracy                           0.80       267\n","   macro avg       0.79      0.79      0.79       267\n","weighted avg       0.80      0.80      0.80       267\n","\n","0.8014981273408239\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MpDloM1hI0OW","colab_type":"text"},"source":["Teste 6"]},{"cell_type":"code","metadata":{"id":"5scQY_QoIyOM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"f8217c80-f1ce-43f4-e515-6b7a356edc28","executionInfo":{"status":"ok","timestamp":1573319342466,"user_tz":180,"elapsed":796,"user":{"displayName":"Duan Nunes Alves da Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfouVJ_YzmdNjtxGrHIL2yKbfjANwHO8v8Y5mSyg=s64","userId":"04410361390824864667"}}},"source":["#Parâmetros do MLP\n","# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’ Activation function for the hidden layer.\n","\n","# ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n","# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n","# ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n","# ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n","# solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n","\n","\n","# The solver for weight optimization.\n","# ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n","# ‘sgd’ refers to stochastic gradient descent.\n","# ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n","\n","\n","mlp_adam =MLPClassifier(alpha=0.0001, max_iter=1000,learning_rate_init=0.01, hidden_layer_sizes=(3,2,2), activation='tanh', solver='adam' )\n","mlp_adam.fit(X_train, y_train)\n","y_pred = mlp_adam.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n","print(metrics.accuracy_score(y_test, y_pred))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.86      0.84       164\n","           1       0.76      0.71      0.73       103\n","\n","    accuracy                           0.80       267\n","   macro avg       0.79      0.78      0.79       267\n","weighted avg       0.80      0.80      0.80       267\n","\n","0.8014981273408239\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7WTlm-9AI5Ws","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}